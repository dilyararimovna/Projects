{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, io\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import string\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#%matplotlib inline\n",
    "import keras.backend.tensorflow_backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 119811\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>you fuck your dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>i really do not understand your point xa it se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a xc xa majority of canadians can and has been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>listen if you dont wanna get married to a man ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>c xe c b u ea n xu u ed ng u u b u eddng bi u ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       1  20120618192155Z                                  you fuck your dad\n",
       "1       0  20120528192215Z  i really do not understand your point xa it se...\n",
       "2       0              NaN  a xc xa majority of canadians can and has been...\n",
       "3       0              NaN  listen if you dont wanna get married to a man ...\n",
       "4       0  20120619094753Z  c xe c b u ea n xu u ed ng u u b u eddng bi u ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = './data/prepr_full_train.csv'\n",
    "train_data_ = pd.read_csv(filename)\n",
    "print ('Data size:', len(train_data_))\n",
    "train_data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 2647\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the drudge report n n n nyou wo not see this s...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120618222256Z</td>\n",
       "      <td>ian xa roger clemens is the fucking man and ne...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618213617Z</td>\n",
       "      <td>agree with alan you are an extremest idiot you...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>really &lt;SIGNS&gt; n ni see marc lamont hill on va...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620003825Z</td>\n",
       "      <td>really suck is not the word when many of our n...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment  \\\n",
       "0       0              NaN  the drudge report n n n nyou wo not see this s...   \n",
       "1       0  20120618222256Z  ian xa roger clemens is the fucking man and ne...   \n",
       "2       1  20120618213617Z  agree with alan you are an extremest idiot you...   \n",
       "3       0              NaN  really <SIGNS> n ni see marc lamont hill on va...   \n",
       "4       0  20120620003825Z  really suck is not the word when many of our n...   \n",
       "\n",
       "         Usage  \n",
       "0   PublicTest  \n",
       "1   PublicTest  \n",
       "2   PublicTest  \n",
       "3  PrivateTest  \n",
       "4  PrivateTest  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = './data/prepr_valid.csv'\n",
    "valid_data_ = pd.read_csv(filename)\n",
    "print ('Data size:', len(valid_data_))\n",
    "valid_data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 2235\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>like this if you are a tribe fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>you idiot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>i am a woman babs and the only war on women i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>wow you benefitted so many wins this year from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>haha green me red you now loser whos winning n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       0  20120603163526Z                   like this if you are a tribe fan\n",
       "1       1  20120531215447Z                                          you idiot\n",
       "2       1  20120823164228Z  i am a woman babs and the only war on women i ...\n",
       "3       1  20120826010752Z  wow you benefitted so many wins this year from...\n",
       "4       1  20120602223825Z  haha green me red you now loser whos winning n..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = './data/prepr_test.csv'\n",
    "test_data_ = pd.read_csv(filename)\n",
    "print ('Data size:', len(test_data_))\n",
    "test_data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data_.dropna(axis=0, how='any')\n",
    "valid_data = valid_data_\n",
    "test_data = test_data_\n",
    "\n",
    "valid_size = len(valid_data)\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "X_train = train_data['Comment']\n",
    "y_train = train_data['Insult']\n",
    "X_valid = valid_data['Comment']\n",
    "y_valid = valid_data['Insult']\n",
    "X_test = test_data['Comment']\n",
    "y_test = test_data['Insult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = './data/embedding_09.vec'\n",
    "\n",
    "vocabulary = []\n",
    "embeddings = []\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    f.readline()\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line_to_list = line.split(' ')\n",
    "        vocabulary.append(line_to_list[0])\n",
    "        embeddings.append(np.array(line_to_list[1:-1]).astype(np.float))\n",
    "        \n",
    "embeddings = np.asarray(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157689\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(vocabulary)\n",
    "embedding_size = embeddings.shape[1]\n",
    "print (vocabulary_size)\n",
    "print (embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledBatchGenerator(object):\n",
    "    def __init__(self, comments, batch_size, comment_size, labels):\n",
    "        self._comments = comments\n",
    "        self._num_comments = len(comments)\n",
    "        self._batch_size = batch_size\n",
    "        self._comment_size = comment_size\n",
    "        self._labels = labels\n",
    "        segment = self._num_comments // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        \n",
    "    def _next_batch(self, step):\n",
    "        batch = np.zeros(shape=(self._batch_size,1), dtype=np.int)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b,0] = self._comments[self._cursor[b]][step]\n",
    "            if step == self._comment_size - 1:\n",
    "                self._cursor[b] = (self._cursor[b] + 1) % self._num_comments\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        batches = []\n",
    "        batches_labels = [self._labels[self._cursor[b]] for b in range(self._batch_size)]\n",
    "        for step in range(0, self._comment_size):\n",
    "            batches.append(self._next_batch(step))\n",
    "        return batches, batches_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comment_size = 100\n",
    "batch_size = 64\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(comment_size)\n",
    "pretrain = vocab_processor.fit(vocabulary)\n",
    "\n",
    "X_train_ = np.array(list(vocab_processor.transform(X_train.as_matrix())))\n",
    "X_valid_ = np.array(list(vocab_processor.transform(X_valid.as_matrix())))\n",
    "X_test_ = np.array(list(vocab_processor.transform(X_test.as_matrix())))\n",
    "\n",
    "train_batches, train_labels = LabeledBatchGenerator(X_train_, train_size, comment_size, y_train.as_matrix()).next()\n",
    "valid_batches, valid_labels = LabeledBatchGenerator(X_valid_, valid_size, comment_size, y_valid.as_matrix()).next()\n",
    "test_batches, test_labels = LabeledBatchGenerator(X_test_, test_size, comment_size, y_test.as_matrix()).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.asarray(train_batches).reshape(comment_size, train_size).T\n",
    "y_train = np.asarray(train_labels).reshape(-1)\n",
    "X_valid = np.asarray(valid_batches).reshape(comment_size, valid_size).T\n",
    "y_valid = np.asarray(valid_labels).reshape(-1)\n",
    "X_test = np.asarray(test_batches).reshape(comment_size, test_size).T\n",
    "y_test = np.asarray(test_labels).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119063, 100)\n",
      "(2647, 100)\n",
      "(2235, 100)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "print (X_valid.shape)\n",
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, concatenate, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers.core import Dropout, Reshape\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated: (?, 300, 20)\n",
      "Reshaped: (?, 6000)\n",
      "Final: (?, 1)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_10 (InputLayer)            (None, 100)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)         (None, 100, 100)      15768900    input_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)               (None, 100, 20)       6020        embedding_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)               (None, 100, 20)       6020        embedding_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)               (None, 100, 20)       6020        embedding_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D)  (None, 100, 20)       0           conv1d_28[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D)  (None, 100, 20)       0           conv1d_29[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling1D)  (None, 100, 20)       0           conv1d_30[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)     (None, 300, 20)       0           max_pooling1d_28[0][0]           \n",
      "                                                                   max_pooling1d_29[0][0]           \n",
      "                                                                   max_pooling1d_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)             (None, 6000)          0           concatenate_10[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 6000)          0           reshape_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 100)           600100      dropout_18[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)             (None, 100)           0           dense_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 1)             101         dropout_19[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 1)             0           dense_19[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 16,387,161\n",
      "Trainable params: 16,387,161\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "(None, 1)\n",
      "Train...\n",
      "Train on 119063 samples, validate on 2647 samples\n",
      "Epoch 1/10\n",
      "119063/119063 [==============================] - 447s - loss: 0.3798 - binary_accuracy: 0.8857 - val_loss: 0.4830 - val_binary_accuracy: 0.8032\n",
      "Epoch 2/10\n",
      "119063/119063 [==============================] - 392s - loss: 0.2753 - binary_accuracy: 0.9026 - val_loss: 0.4691 - val_binary_accuracy: 0.8028\n",
      "Epoch 3/10\n",
      "119063/119063 [==============================] - 235s - loss: 0.2656 - binary_accuracy: 0.9062 - val_loss: 0.4640 - val_binary_accuracy: 0.8168\n",
      "Epoch 4/10\n",
      "119063/119063 [==============================] - 387s - loss: 0.2601 - binary_accuracy: 0.9088 - val_loss: 0.4625 - val_binary_accuracy: 0.8134\n",
      "Epoch 5/10\n",
      " 68928/119063 [================>.............] - ETA: 190s - loss: 0.2586 - binary_accuracy: 0.9102"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1cdb4867ea08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m               verbose=1)\n\u001b[0m\u001b[1;32m     54\u001b[0m     score, acc = model.evaluate(X_valid, y_valid.reshape(-1,1),\n\u001b[1;32m     55\u001b[0m                                 batch_size=batch_size)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "patch_size = [3,3,3]\n",
    "num_filters = 20\n",
    "regul_coef = 0.01\n",
    "\n",
    "with K.tf.device('/gpu:6'):\n",
    "    K.set_session(K.tf.Session(config=K.tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)))\n",
    "\n",
    "    inputs = Input(shape=(comment_size,))\n",
    "    embed_inputs = Embedding(vocabulary_size, embedding_size, weights=[embeddings], trainable=True)(inputs)\n",
    "\n",
    "    output_0 = Conv1D(num_filters, kernel_size=patch_size[0], activation='relu', \n",
    "                          kernel_regularizer=l2(regul_coef), padding='same')(embed_inputs)\n",
    "    output_0 = MaxPooling1D(pool_size=2, strides=1, padding='same')(output_0)\n",
    "\n",
    "    output_1 = Conv1D(num_filters, kernel_size=patch_size[1], activation='relu', \n",
    "                          kernel_regularizer=l2(regul_coef), padding='same')(embed_inputs)\n",
    "    output_1 = MaxPooling1D(pool_size=2, strides=1, padding='same')(output_1)\n",
    "\n",
    "    output_2 = Conv1D(num_filters, kernel_size=patch_size[2], activation='relu', \n",
    "                          kernel_regularizer=l2(regul_coef), padding='same')(embed_inputs)\n",
    "    output_2 = MaxPooling1D(pool_size=2, strides=1, padding='same')(output_2)\n",
    "\n",
    "    output = concatenate([output_0, output_1, output_2], axis=1)\n",
    "    print('Concatenated:', output.shape)\n",
    "\n",
    "    output = Reshape(((comment_size * len(patch_size) ) * num_filters,))(output)\n",
    "    print ('Reshaped:', output.shape)\n",
    "\n",
    "    output = Dropout(rate=0.5)(output)\n",
    "    output = Dense(100, activation='relu', kernel_regularizer=l2(regul_coef))(output) \n",
    "\n",
    "    output = Dropout(rate=0.5)(output)\n",
    "    output = Dense(1, activation=None, kernel_regularizer=l2(regul_coef))(output)\n",
    "\n",
    "    act_output = Activation('sigmoid')(output)\n",
    "    print ('Final:',act_output.shape)\n",
    "    model = Model(inputs=inputs, outputs=act_output)\n",
    "\n",
    "    optimizer = Adam(lr=0.01, decay=0.1)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['binary_accuracy'])\n",
    "\n",
    "    print (model.summary())\n",
    "    print (model.output_shape)\n",
    "\n",
    "    print('Train...')\n",
    "    model.fit(X_train, y_train.reshape(-1,1),\n",
    "              batch_size=batch_size,\n",
    "              epochs=10,\n",
    "              validation_data=(X_valid, y_valid.reshape(-1,1)),\n",
    "              verbose=1)\n",
    "    score, acc = model.evaluate(X_valid, y_valid.reshape(-1,1),\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    model.save_weights(filepath='./checkpoint_dir/wordCNNkeras_reddit')\n",
    "\n",
    "    print('Valid score:', score)\n",
    "    print('Valid accuracy:', acc)\n",
    "    \n",
    "    y_train_pred = np.round(model.predict(X_train))\n",
    "    print ('Train AUC-ROC:',roc_auc_score(y_train, y_train_pred.reshape(-1)))\n",
    "\n",
    "    y_valid_pred = np.round(model.predict(X_valid))\n",
    "    print ('Valid AUC-ROC:',roc_auc_score(y_valid, y_valid_pred.reshape(-1)))\n",
    "\n",
    "    y_test_pred = np.round(model.predict(X_test))\n",
    "    print ('Test AUC-ROC:',roc_auc_score(y_test, y_test_pred.reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
