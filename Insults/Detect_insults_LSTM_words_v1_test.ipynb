{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import string\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 3947\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>you fuck your dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>i really do not understand your point xa it se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a xc xa majority of canadians can and has been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>listen if you dont wanna get married to a man ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>c xe c b u ea n xu u ed ng u u b u eddng bi u ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       1  20120618192155Z                                  you fuck your dad\n",
       "1       0  20120528192215Z  i really do not understand your point xa it se...\n",
       "2       0              NaN  a xc xa majority of canadians can and has been...\n",
       "3       0              NaN  listen if you dont wanna get married to a man ...\n",
       "4       0  20120619094753Z  c xe c b u ea n xu u ed ng u u b u eddng bi u ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = './data/prepr_train.csv'\n",
    "train_data_ = pd.read_csv(filename)\n",
    "print ('Data size:', len(train_data_))\n",
    "train_data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 2647\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the drudge report n n n nyou wo not see this s...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120618222256Z</td>\n",
       "      <td>ian xa roger clemens is the fucking man and ne...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618213617Z</td>\n",
       "      <td>agree with alan you are an extremest idiot you...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>really &lt;SIGNS&gt; n ni see marc lamont hill on va...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620003825Z</td>\n",
       "      <td>really suck is not the word when many of our n...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment  \\\n",
       "0       0              NaN  the drudge report n n n nyou wo not see this s...   \n",
       "1       0  20120618222256Z  ian xa roger clemens is the fucking man and ne...   \n",
       "2       1  20120618213617Z  agree with alan you are an extremest idiot you...   \n",
       "3       0              NaN  really <SIGNS> n ni see marc lamont hill on va...   \n",
       "4       0  20120620003825Z  really suck is not the word when many of our n...   \n",
       "\n",
       "         Usage  \n",
       "0   PublicTest  \n",
       "1   PublicTest  \n",
       "2   PublicTest  \n",
       "3  PrivateTest  \n",
       "4  PrivateTest  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = './data/prepr_valid.csv'\n",
    "valid_data_ = pd.read_csv(filename)\n",
    "print ('Data size:', len(valid_data_))\n",
    "valid_data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 2235\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>like this if you are a tribe fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>you idiot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>i am a woman babs and the only war on women i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>wow you benefitted so many wins this year from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>haha green me red you now loser whos winning n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       0  20120603163526Z                   like this if you are a tribe fan\n",
       "1       1  20120531215447Z                                          you idiot\n",
       "2       1  20120823164228Z  i am a woman babs and the only war on women i ...\n",
       "3       1  20120826010752Z  wow you benefitted so many wins this year from...\n",
       "4       1  20120602223825Z  haha green me red you now loser whos winning n..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = './data/prepr_test.csv'\n",
    "test_data_ = pd.read_csv(filename)\n",
    "print ('Data size:', len(test_data_))\n",
    "test_data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data_\n",
    "valid_data = valid_data_\n",
    "test_data = test_data_\n",
    "                               \n",
    "valid_size = len(valid_data)\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "X_train = train_data['Comment']\n",
    "y_train = train_data['Insult']\n",
    "X_valid = valid_data['Comment']\n",
    "y_valid = valid_data['Insult']\n",
    "X_test = test_data['Comment']\n",
    "y_test = test_data['Insult']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data in words: 728927\n",
      "Most common words (+UNK) [['UNK', 0], ['<EOS>', 0], ['<PAD>', 0], ['<SIGNS>', 1747], ('the', 4780)]\n",
      "Sample data [5, 74, 19, 708, 10, 98, 23, 13, 255, 19]\n",
      "Dict: 0\n",
      "Reverse dict: UNK\n",
      "Counter: ['UNK', 0]\n",
      "Data: 5\n",
      "Words: you fuck y\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_vocabulary(words):\n",
    "    counter_words = collections.Counter(words)\n",
    "    count = [['UNK', -3], ['<EOS>', 0], ['<PAD>', 0], ['<SIGNS>', counter_words['<SIGNS>']]]\n",
    "    count.extend(counter_words.most_common(vocabulary_size - 4))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "words = ' '.join(X_train)\n",
    "print ('Length of train data in words:', len(words))\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_vocabulary(words.split(' '))\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "\n",
    "print ('Dict:', dictionary['UNK'])\n",
    "print ('Reverse dict:', reverse_dictionary[0])\n",
    "print ('Counter:', count[dictionary['UNK']])\n",
    "print ('Data:', data[0])\n",
    "print ('Words:', words[:10])\n",
    "del words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2id(word):\n",
    "    if word in dictionary:\n",
    "        return dictionary[word]\n",
    "    else:\n",
    "        return 0 # UNK\n",
    "\n",
    "def id2word(id_):\n",
    "    return reverse_dictionary[id_]\n",
    "\n",
    "\n",
    "def comment2vec(comment):\n",
    "    global comment_size\n",
    "    split = comment.split(' ')\n",
    "    res = np.array([word2id(word) for word in split], dtype='int')\n",
    "    return res\n",
    "\n",
    "def vec2comment(vec):\n",
    "    global comment_size\n",
    "    return ' '.join([id2word(id_) for id_ in vec])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabeledBatchGenerator(object):\n",
    "    def __init__(self, comments, batch_size, comment_size, labels):\n",
    "        self._comments = comments\n",
    "        self._num_comments = len(comments)\n",
    "        self._batch_size = batch_size\n",
    "        self._comment_size = comment_size\n",
    "        self._labels = labels\n",
    "        segment = self._num_comments // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        \n",
    "    def _next_batch(self, step):\n",
    "        batch = np.zeros(shape=(self._batch_size,1), dtype=np.int)\n",
    "        for b in range(self._batch_size):\n",
    "            comment = comment2vec(self._comments[self._cursor[b]])\n",
    "            N = len(comment)\n",
    "            if step < N:\n",
    "                batch[b,0] = comment[step]\n",
    "            elif step == N:\n",
    "                batch[b,0] = word2id('<EOS>')\n",
    "            elif step > N:\n",
    "                batch[b,0] = word2id('<PAD>')\n",
    "            if step == self._comment_size - 1:\n",
    "                if N > self._comment_size - 1:\n",
    "                    batch[b,0] = word2id('<EOS>')\n",
    "                self._cursor[b] = (self._cursor[b] + 1) % self._num_comments\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        batches = []\n",
    "        batches_labels = [self._labels[self._cursor[b]] for b in range(self._batch_size)]\n",
    "        for step in range(0, self._comment_size):\n",
    "            batches.append(self._next_batch(step))\n",
    "        return batches, batches_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comment_size = 100\n",
    "batch_size = test_size\n",
    "test_batch_generator = LabeledBatchGenerator(X_test.as_matrix(), batch_size, comment_size, y_test.as_matrix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(losses_, mean_losses_, train_aucs_, valid_aucs_, frequency):\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(losses_)\n",
    "    plt.title('Losses')\n",
    "    plt.xlabel('Step')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(frequency * np.arange(1,len(mean_losses_)+1), mean_losses_)\n",
    "    plt.title('MeanLosses')\n",
    "    plt.xlabel('Step')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.plot(train_aucs_, label='Train AUC-ROC')\n",
    "    plt.plot(frequency * np.arange(1,len(valid_aucs_)+1), valid_aucs_, label='Valid AUC-ROC')\n",
    "    plt.xlabel('Step')\n",
    "    plt.title('AUC-ROC')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After embedding shape: (64, 100, 128)\n",
      "Final encod state shape: (64, 6400)\n",
      "Logits and labels shape: (64, 1) (64, 1)\n",
      "Graph is ready!\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "num_nodes = 32\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_data = list()\n",
    "    for _ in range(comment_size):\n",
    "        tf_data.append(tf.placeholder(tf.int32, shape=[batch_size, 1])) \n",
    "    tf_labels = tf.placeholder(tf.float32, shape=[batch_size,1])\n",
    "    \n",
    "    tf_encod_ifcox_forward = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1), name='x_f')\n",
    "    tf_encod_ifcom_forward = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1), name='m_f')\n",
    "    tf_encod_ifcob_forward = tf.Variable(tf.zeros([1, 4 * num_nodes]), name='b_f')\n",
    "    tf_encod_saved_output_forward = tf.Variable(tf.fill([batch_size, num_nodes],0.0), \n",
    "                                                trainable=False, validate_shape=False, name='saved_output_f')\n",
    "    tf_encod_saved_state_forward = tf.Variable(tf.fill([batch_size, num_nodes],0.0),\n",
    "                                               trainable=False, validate_shape=False, name='saved_state_f')\n",
    "\n",
    "    tf_encod_ifcox_backward = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1), name='x_b')\n",
    "    tf_encod_ifcom_backward = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1), name='m_b')\n",
    "    tf_encod_ifcob_backward = tf.Variable(tf.zeros([1, 4 * num_nodes]), name='b_b')\n",
    "    tf_encod_saved_output_backward = tf.Variable(tf.fill([batch_size, num_nodes], 0.0), \n",
    "                                                 trainable=False, validate_shape=False, name='saved_output_b')\n",
    "    tf_encod_saved_state_backward = tf.Variable(tf.fill([batch_size, num_nodes],0.0), \n",
    "                                                trainable=False, validate_shape=False, name='saved_state_b')\n",
    "\n",
    "    tf_embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), name='embeddings')\n",
    "    tf_keep_prob = tf.placeholder(tf.float32) \n",
    "    \n",
    "    tf_encod_inputs_forward = tf.concat(tf_data[:comment_size], axis=1)\n",
    "    tf_encod_inputs_backward = tf.concat(tf_data[:comment_size], axis=1)[::-1]\n",
    "    tf_embedded_encod_inputs_forward = tf.nn.embedding_lookup(tf_embeddings, tf_encod_inputs_forward) \n",
    "    tf_embedded_encod_inputs_backward = tf.nn.embedding_lookup(tf_embeddings, tf_encod_inputs_backward) \n",
    "    print ('After embedding shape:', tf_embedded_encod_inputs_forward.shape)\n",
    "    \n",
    "    def encod_lstm_cell_forward(tf_i, tf_o, tf_state):\n",
    "        tf_summ = tf.matmul(tf_i, tf_encod_ifcox_forward) + tf.matmul(tf_o, tf_encod_ifcom_forward) + tf_encod_ifcob_forward\n",
    "        tf_state = (tf.sigmoid(tf_summ[:,num_nodes:2*num_nodes]) * tf_state +\n",
    "                 tf.sigmoid(tf_summ[:,:num_nodes]) * tf.tanh(tf_summ[:,2*num_nodes:3*num_nodes]))\n",
    "        return tf.sigmoid(tf_summ[:,3*num_nodes:]) * tf.tanh(tf_state), tf_state\n",
    "    \n",
    "    def encod_lstm_cell_backward(tf_i, tf_o, tf_state):\n",
    "        tf_summ = tf.matmul(tf_i, tf_encod_ifcox_backward) + tf.matmul(tf_o, tf_encod_ifcom_backward) + tf_encod_ifcob_backward\n",
    "        tf_state = (tf.sigmoid(tf_summ[:,num_nodes:2*num_nodes]) * tf_state +\n",
    "                 tf.sigmoid(tf_summ[:,:num_nodes]) * tf.tanh(tf_summ[:,2*num_nodes:3*num_nodes]))\n",
    "        return tf.sigmoid(tf_summ[:,3*num_nodes:]) * tf.tanh(tf_state), tf_state\n",
    "    \n",
    "    def encod_lstm_cell_up(tf_i, tf_o, tf_state):\n",
    "        tf_summ = tf.matmul(tf_i, tf_encod_ifcox_up) + tf.matmul(tf_o, tf_encod_ifcom_up) + tf_encod_ifcob_up\n",
    "        tf_state = (tf.sigmoid(tf_summ[:,num_nodes:2*num_nodes]) * tf_state +\n",
    "                 tf.sigmoid(tf_summ[:,:num_nodes]) * tf.tanh(tf_summ[:,2*num_nodes:3*num_nodes]))\n",
    "        return tf.sigmoid(tf_summ[:,3*num_nodes:]) * tf.tanh(tf_state), tf_state\n",
    "    \n",
    "    tf_encod_states_forward = list()\n",
    "    tf_encod_states_backward = list()\n",
    "\n",
    "    tf_encod_state_forward = tf_encod_saved_state_forward\n",
    "    tf_encod_output_forward = tf_encod_saved_output_forward\n",
    "    \n",
    "    for tf_i in tf.unstack(tf_embedded_encod_inputs_forward, axis=1):#comment_size\n",
    "        tf_encod_output_forward, tf_encod_state_forward = encod_lstm_cell_forward(tf_i, \n",
    "                                                                                  tf_encod_output_forward, \n",
    "                                                                                  tf_encod_state_forward)\n",
    "        #shape is questionable\n",
    "        tf_encod_states_forward.append(tf.reshape(tf_encod_state_forward, shape=[batch_size, num_nodes]))\n",
    "        \n",
    "    tf_encod_state_backward = tf_encod_saved_state_backward\n",
    "    tf_encod_output_backward = tf_encod_saved_output_backward\n",
    "    \n",
    "    for tf_i in tf.unstack(tf_embedded_encod_inputs_backward, axis=1): #comment_size\n",
    "        tf_encod_output_backward, tf_encod_state_backward = encod_lstm_cell_backward(tf_i, \n",
    "                                                                                     tf_encod_output_backward, \n",
    "                                                                                     tf_encod_state_backward)\n",
    "        tf_encod_states_backward.append(tf.reshape(tf_encod_state_backward, shape=[batch_size, num_nodes]))\n",
    "        \n",
    "    tf_encod_inputs_up = tf.concat([tf_encod_states_forward, tf_encod_states_backward], axis=2)\n",
    "\n",
    "    tf_encod_state_up_reshape = []\n",
    "    for tf_i in tf.unstack(tf_encod_inputs_up,axis=1):\n",
    "        tf_encod_state_up_reshape.append(tf.reshape(tf_i, shape=[1,-1]))\n",
    "    tf_encod_states_up = tf.concat(tf_encod_state_up_reshape, axis=0)\n",
    "    print ('Final encod state shape:', tf_encod_states_up.shape)\n",
    "    \n",
    "    with tf.control_dependencies([tf_encod_saved_output_forward.assign(tf_encod_output_forward),\n",
    "                                  tf_encod_saved_output_backward.assign(tf_encod_output_backward),\n",
    "                                  tf_encod_saved_state_forward.assign(tf_encod_state_forward),\n",
    "                                  tf_encod_saved_state_backward.assign(tf_encod_state_backward)]):\n",
    "\n",
    "        tf_logits_ = tf.layers.dense(inputs=tf_encod_states_up, units=num_nodes, \n",
    "                                     activation=None, name='dense1')#kernel_regularizer=tf.nn.l2_loss\n",
    "        tf_logits = tf.layers.dense(inputs=tf_logits_, units=1, activation=None, name='dense2')\n",
    "        print ('Logits and labels shape:', tf_logits.shape, tf_labels.shape)\n",
    "        tf_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=tf_labels, logits=tf_logits))\n",
    "\n",
    "    tf_global_step = tf.Variable(0, name='global_step')\n",
    "    tf_learning_rate = tf.train.exponential_decay(0.0001, tf_global_step, 1000, 0.9, staircase=False)\n",
    "    tf_optimizer = tf.train.AdamOptimizer(tf_learning_rate)\n",
    "    tf_grads = tf_optimizer.compute_gradients(tf_loss)\n",
    "    tf_optimizer = tf_optimizer.apply_gradients(tf_grads, global_step=tf_global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    tf_prediction = tf.round(tf.sigmoid(tf_logits))\n",
    "    tf_auc_roc, tf_update_oper = tf.metrics.auc(tf_labels, tf_prediction)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    print('Graph is ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint_dir/wordLSTM-10000\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key b_b not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-de09d0df69dc>\", line 106, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1056, in __init__\n    self.build()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 669, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key b_b not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key b_b not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-bbbc99db81d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./checkpoint_dir/wordLSTM-10000\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model restored.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mbatches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_batch_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restoring parameters from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1457\u001b[1;33m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key b_b not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-de09d0df69dc>\", line 106, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1056, in __init__\n    self.build()\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 669, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Dilyara\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key b_b not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, \"./checkpoint_dir/wordLSTM-10000\")\n",
    "    print(\"Model restored.\")\n",
    "    batches, labels = test_batch_generator.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(comment_size):\n",
    "        feed_dict[tf_data[i]] = batches[i]\n",
    "    labels_ = np.asarray(labels).astype('float').reshape(-1,1)\n",
    "    feed_dict[tf_labels] = labels_\n",
    "    feed_dict[tf_keep_prob] = 1.\n",
    "    update, auc, predictions = session.run([tf_update_oper, tf_auc_roc,\n",
    "                                               tf_prediction],\n",
    "                                              feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
