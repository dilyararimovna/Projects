{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import string\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 3947\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>you fuck your dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>i really do not understand your point xa it se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a xc xa majority of canadians can and has been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>listen if you dont wanna get married to a man ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>c xe c b u ea n xu u ed ng u u b u eddng bi u ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       1  20120618192155Z                                  you fuck your dad\n",
       "1       0  20120528192215Z  i really do not understand your point xa it se...\n",
       "2       0              NaN  a xc xa majority of canadians can and has been...\n",
       "3       0              NaN  listen if you dont wanna get married to a man ...\n",
       "4       0  20120619094753Z  c xe c b u ea n xu u ed ng u u b u eddng bi u ..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = './data/prepr_train.csv'\n",
    "train_data_ = pd.read_csv(filename)\n",
    "print ('Data size:', len(train_data_))\n",
    "train_data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 2647\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the drudge report n n n nyou wo not see this s...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120618222256Z</td>\n",
       "      <td>ian xa roger clemens is the fucking man and ne...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618213617Z</td>\n",
       "      <td>agree with alan you are an extremest idiot you...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>really &lt;SIGNS&gt; n ni see marc lamont hill on va...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620003825Z</td>\n",
       "      <td>really suck is not the word when many of our n...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment  \\\n",
       "0       0              NaN  the drudge report n n n nyou wo not see this s...   \n",
       "1       0  20120618222256Z  ian xa roger clemens is the fucking man and ne...   \n",
       "2       1  20120618213617Z  agree with alan you are an extremest idiot you...   \n",
       "3       0              NaN  really <SIGNS> n ni see marc lamont hill on va...   \n",
       "4       0  20120620003825Z  really suck is not the word when many of our n...   \n",
       "\n",
       "         Usage  \n",
       "0   PublicTest  \n",
       "1   PublicTest  \n",
       "2   PublicTest  \n",
       "3  PrivateTest  \n",
       "4  PrivateTest  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = './data/prepr_valid.csv'\n",
    "valid_data_ = pd.read_csv(filename)\n",
    "print ('Data size:', len(valid_data_))\n",
    "valid_data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 2235\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>like this if you are a tribe fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>you idiot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>i am a woman babs and the only war on women i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>wow you benefitted so many wins this year from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>haha green me red you now loser whos winning n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       0  20120603163526Z                   like this if you are a tribe fan\n",
       "1       1  20120531215447Z                                          you idiot\n",
       "2       1  20120823164228Z  i am a woman babs and the only war on women i ...\n",
       "3       1  20120826010752Z  wow you benefitted so many wins this year from...\n",
       "4       1  20120602223825Z  haha green me red you now loser whos winning n..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = './data/prepr_test.csv'\n",
    "test_data_ = pd.read_csv(filename)\n",
    "print ('Data size:', len(test_data_))\n",
    "test_data_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data_\n",
    "valid_data = valid_data_\n",
    "test_data = test_data_\n",
    "\n",
    "valid_size = len(valid_data)\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "X_train = train_data['Comment']\n",
    "y_train = train_data['Insult']\n",
    "X_valid = valid_data['Comment']\n",
    "y_valid = valid_data['Insult']\n",
    "X_test = test_data['Comment']\n",
    "y_test = test_data['Insult']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data in words: 728927\n",
      "Most common words (+UNK) [['UNK', 0], ['<EOS>', 0], ['<PAD>', 0], ['<SIGNS>', 1747], ('the', 4780)]\n",
      "Sample data [5, 74, 19, 707, 10, 98, 23, 13, 254, 19]\n",
      "Dict: 0\n",
      "Reverse dict: UNK\n",
      "Counter: ['UNK', 0]\n",
      "Data: 5\n",
      "Words: you fuck y\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_vocabulary(words):\n",
    "    counter_words = collections.Counter(words)\n",
    "    count = [['UNK', -3], ['<EOS>', 0], ['<PAD>', 0], ['<SIGNS>', counter_words['<SIGNS>']]]\n",
    "    count.extend(counter_words.most_common(vocabulary_size - 4))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "words = ' '.join(X_train)\n",
    "print ('Length of train data in words:', len(words))\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_vocabulary(words.split(' '))\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "\n",
    "print ('Dict:', dictionary['UNK'])\n",
    "print ('Reverse dict:', reverse_dictionary[0])\n",
    "print ('Counter:', count[dictionary['UNK']])\n",
    "print ('Data:', data[0])\n",
    "print ('Words:', words[:10])\n",
    "del words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2id(word):\n",
    "    if word in dictionary:\n",
    "        return dictionary[word]\n",
    "    else:\n",
    "        return 0 # UNK\n",
    "\n",
    "def id2word(id_):\n",
    "    return reverse_dictionary[id_]\n",
    "\n",
    "\n",
    "def comment2vec(comment):\n",
    "    global comment_size\n",
    "    split = comment.split(' ')\n",
    "    res = np.array([word2id(word) for word in split], dtype='int')\n",
    "    return res\n",
    "\n",
    "def vec2comment(vec):\n",
    "    global comment_size\n",
    "    return ' '.join([id2word(id_) for id_ in vec])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabeledBatchGenerator(object):\n",
    "    def __init__(self, comments, batch_size, comment_size, labels):\n",
    "        self._comments = comments\n",
    "        self._num_comments = len(comments)\n",
    "        self._batch_size = batch_size\n",
    "        self._comment_size = comment_size\n",
    "        self._labels = labels\n",
    "        segment = self._num_comments // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        \n",
    "    def _next_batch(self, step):\n",
    "        batch = np.zeros(shape=(self._batch_size,1), dtype=np.int)\n",
    "        for b in range(self._batch_size):\n",
    "            comment = comment2vec(self._comments[self._cursor[b]])\n",
    "            N = len(comment)\n",
    "            if step < N:\n",
    "                batch[b,0] = comment[step]\n",
    "            elif step == N:\n",
    "                batch[b,0] = word2id('<EOS>')\n",
    "            elif step > N:\n",
    "                batch[b,0] = word2id('<PAD>')\n",
    "            if step == self._comment_size - 1:\n",
    "                if N > self._comment_size - 1:\n",
    "                    batch[b,0] = word2id('<EOS>')\n",
    "                self._cursor[b] = (self._cursor[b] + 1) % self._num_comments\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        batches = []\n",
    "        batches_labels = [self._labels[self._cursor[b]] for b in range(self._batch_size)]\n",
    "        for step in range(0, self._comment_size):\n",
    "            batches.append(self._next_batch(step))\n",
    "        return batches, batches_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comment_size = 100\n",
    "batch_size = 64\n",
    "valid_batch_size = valid_size\n",
    "test_batch_size = test_size\n",
    "\n",
    "train_batch_generator = LabeledBatchGenerator(X_train.as_matrix(), batch_size, comment_size, y_train.as_matrix())\n",
    "valid_batch_generator = LabeledBatchGenerator(X_valid.as_matrix(), valid_batch_size, comment_size, y_valid.as_matrix())\n",
    "test_batch_generator = LabeledBatchGenerator(X_test.as_matrix(), test_batch_size, comment_size, y_test.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(losses_, mean_losses_, train_aucs_, valid_aucs_, frequency):\n",
    "    plt.plot(losses_)\n",
    "    plt.title('Losses')\n",
    "    plt.xlabel('Step')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(frequency * np.arange(1,len(mean_losses_)+1), mean_losses_)\n",
    "    plt.title('MeanLosses')\n",
    "    plt.xlabel('Step')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_aucs_, label='Train AUC-ROC')\n",
    "    plt.plot(frequency * np.arange(1,len(valid_aucs_)+1), valid_aucs_, label='Valid AUC-ROC')\n",
    "    plt.xlabel('Step')\n",
    "    plt.title('AUC-ROC')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100, 93)\n",
      "Graph is ready!\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "patch_size = [3,4,5]\n",
    "num_filters = 32\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), name='embed')\n",
    "    tf_keep_prob = tf.placeholder(tf.float32)\n",
    "    tf_data = list()\n",
    "    tf_labels = tf.placeholder(tf.float32, shape=[None,1])\n",
    "    for _ in range(comment_size):\n",
    "        tf_data.append(tf.placeholder(tf.int32, shape=[None, 1]))  \n",
    "    tf_inputs = tf.concat(tf_data[:comment_size], axis=1)    \n",
    "    tf_embedded_inputs = tf.nn.embedding_lookup(tf_embeddings, tf_inputs) # batch_size, comment_size, embed_size\n",
    "\n",
    "    tf_w0 = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size[0], embedding_size, num_filters], stddev=0.1), name='w0')\n",
    "    tf_w1 = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size[1], embedding_size, num_filters], stddev=0.1), name='w1')\n",
    "    tf_w2 = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size[2], embedding_size, num_filters], stddev=0.1), name='w2')\n",
    "    \n",
    "    def multi_layer_cnn(data):\n",
    "        output = []\n",
    "        conv = tf.nn.conv1d(data, tf_w0, 1, padding='SAME')\n",
    "        conv = tf.nn.relu(conv)\n",
    "        pool = tf.nn.max_pool(tf.expand_dims(conv,-1), [1, 1, 2, 1], [1, 1, 1, 1], 'VALID')\n",
    "        pool = tf.nn.relu(pool)\n",
    "        pool = tf.squeeze(pool, axis=3)\n",
    "        output.append(pool)\n",
    "        \n",
    "        conv = tf.nn.conv1d(data, tf_w1, 1, padding='SAME')\n",
    "        conv = tf.nn.relu(conv)\n",
    "        pool = tf.nn.max_pool(tf.expand_dims(conv,-1), [1, 1, 2, 1], [1, 1, 1, 1], 'VALID')\n",
    "        pool = tf.nn.relu(pool)\n",
    "        pool = tf.squeeze(pool, axis=3)\n",
    "        output.append(pool)\n",
    "        \n",
    "        conv = tf.nn.conv1d(data, tf_w2, 1, padding='SAME')\n",
    "        conv = tf.nn.relu(conv)\n",
    "        pool = tf.nn.max_pool(tf.expand_dims(conv,-1), [1, 1, 2, 1], [1, 1, 1, 1], 'VALID')\n",
    "        pool = tf.nn.relu(pool)\n",
    "        pool = tf.squeeze(pool, axis=3)\n",
    "        output.append(pool)\n",
    "        \n",
    "        output = tf.concat(output, axis=2)\n",
    "        print (output.shape)\n",
    "        \n",
    "        shape = output.get_shape().as_list()\n",
    "        reshape = tf.reshape(output, [-1, shape[1] * shape[2]])\n",
    "        return tf.layers.dense(inputs=reshape, units=1, activation=None)\n",
    "\n",
    "    #tf_logits = tf.nn.dropout(multi_layer_cnn(tf_embedded_inputs), tf_keep_prob)\n",
    "    tf_logits = multi_layer_cnn(tf_embedded_inputs)\n",
    "    tf_loss = (tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf_logits, labels=tf_labels)) )\n",
    "    tf_global_step = tf.Variable(0, trainable=False, name='step')\n",
    "    tf_learning_rate = tf.train.exponential_decay(0.01, tf_global_step,\n",
    "                                               1000, 0.9, staircase=True)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    tf_optimizer = tf.train.AdamOptimizer(tf_learning_rate)\n",
    "    tf_grads = tf_optimizer.compute_gradients(tf_loss)\n",
    "    tf_op = tf_optimizer.apply_gradients(tf_grads, global_step=tf_global_step)\n",
    "\n",
    "    tf_prediction = tf.round(tf.sigmoid(tf_logits))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    print('Graph is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint_dir/wordCNN-8000\n",
      "Restored\n",
      "Test predicted: [1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1\n",
      " 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0]\n",
      "Test predicted: [1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1\n",
      " 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0]\n",
      "Test AUC-ROC:  0.58048\n",
      "TIME: 0:00:05.650522\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.datetime.now()\n",
    "losses = []\n",
    "train_aucs = []\n",
    "valid_aucs = []\n",
    "num_steps = 101\n",
    "frequency = 10\n",
    "mean_losses = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    tf.local_variables_initializer().run()\n",
    "    saver.restore(session, './checkpoint_dir/wordCNN-8000')\n",
    "    print('Restored')\n",
    "    \n",
    "    test_b, test_lab = test_batch_generator.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(comment_size):\n",
    "        feed_dict[tf_data[i]] = test_b[i]\n",
    "    feed_dict[tf_keep_prob] = 1.\n",
    "    feed_dict[tf_labels] = np.asarray(test_lab).astype('float').reshape(-1,1)\n",
    "    test_pred = session.run([tf_prediction], feed_dict=feed_dict)\n",
    "    test_auc = roc_auc_score(np.asarray(test_lab).reshape(-1), np.asarray(test_pred).reshape(-1))\n",
    "    print('Test predicted:', np.asarray(test_pred).astype('int').reshape(-1)[:100])\n",
    "    print('Test predicted:', np.asarray(test_lab).astype('int').reshape(-1)[:100])\n",
    "    print('Test AUC-ROC:  %.5f' % test_auc)\n",
    "t1 = datetime.datetime.now()\n",
    "print ('TIME:', t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.580482870765\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print (roc_auc_score(np.asarray(test_lab).reshape(-1), np.asarray(test_pred).reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
